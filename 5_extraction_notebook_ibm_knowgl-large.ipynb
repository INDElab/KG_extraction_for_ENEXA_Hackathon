{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Extraction Notebook - IBM Knowgl-large\n",
    "\n",
    "##### In this notebook, we:\n",
    "1. use the latest version of Python, Pytorch and Transformers like the Extraction Notebook - REBEL.\n",
    "2. load the data that we preprocessed in the previous notebooks.\n",
    "3. set the generation parameters for the model\n",
    "4. define a post-processing function to shape the triples from the IBM/knowgl-large output\n",
    "5. execute the extraction\n",
    "6. use the function to get wikidata IDs.\n",
    "7. make an entity dict: \n",
    "    * entity_dict[KnowGL_extracted_string_in_subject_or_object_position] = wikidata_id_if_any\n",
    "8. make a relation dict: \n",
    "    * relation_dict[KnowGL_extracted_string_in_predicate_position] = wikidata_id_if_any\n",
    "8. make a type dict: \n",
    "    * type_dict[KnowGL_extracted_string_in_subject_or_object_position_with_type_details] = wikidata_id_if_any\n",
    "9. convert KnowGL triples into Wikidata triples.\n",
    "10. save all the extracted triples, entities, relations and types separately in a jsonl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.9\n",
      "Pythorch version:  2.0.1\n",
      "Transformers version:  4.30.1\n"
     ]
    }
   ],
   "source": [
    "#KnowGL environment details:\n",
    "!python --version\n",
    "import torch\n",
    "print('Pythorch version: ', torch.__version__)\n",
    "import transformers\n",
    "print('Transformers version: ', transformers.__version__)\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data instances:  917\n"
     ]
    }
   ],
   "source": [
    "# Let's start by loading the data we preprocessed in the previous notebook.\n",
    "data = []\n",
    "with open('data/preprocessed_data_for_extraction.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.rstrip('None\\n')))\n",
    "print('Number of data instances: ', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we call the tokenizer and the model from the HuggingFace library\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ibm/knowgl-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ibm/knowgl-large\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set the generation parameters for the model\n",
    "gen_kwargs = {\n",
    "    \"max_length\": 1024,\n",
    "    \"length_penalty\": 0,\n",
    "    \"num_beams\": 10, # 10 beams is NOT the default value but we opted for it to get more diverse results\n",
    "    \"num_return_sequences\": 10, # 10 sequences is NOT the default value but we opted for it to get long tail triple extraction\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a post-processing function to shape the triples from the KnowGL output\n",
    "def extract_triples(decoded_preds):\n",
    "    \"\"\" decoded_preds: list of strings, each string is a decoded prediction from KnowGL. len(decoded_preds) = num_return_sequences\"\"\"\n",
    "    triples = set()\n",
    "    entity_set = set()\n",
    "    entity_triples = set()\n",
    "    relation_set = set()\n",
    "    for pred in decoded_preds:\n",
    "        pred = pred.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"(\", \"\").replace(\")\", \"\") # we remove the brackets and parenthesisstrip() # remove special tokens\n",
    "        if pred == '':\n",
    "            continue\n",
    "        \n",
    "        if '$' in pred:\n",
    "            pred = pred.split('$')\n",
    "        else:\n",
    "            pred = [pred]\n",
    "            \n",
    "        pred = [triple.split('|') for triple in pred]   \n",
    "        \n",
    "        for triple in pred:\n",
    "            if len(triple) != 3:\n",
    "                continue\n",
    "            if triple[0] == '' or triple[1] == '' or triple[2] == '':\n",
    "                continue\n",
    "            \n",
    "            sbj = triple[0].split('#')\n",
    "            if len(sbj) != 3:\n",
    "                continue\n",
    "            entity_set.add(sbj[0])\n",
    "            entity_triples.add((sbj[0], \"label\", sbj[1]))\n",
    "            entity_triples.add((sbj[0], \"type\", sbj[2]))\n",
    "            \n",
    "            rel = triple[1]\n",
    "            relation_set.add(rel)\n",
    "            \n",
    "            obj = triple[2].split('#')\n",
    "            if len(obj) != 3:\n",
    "                continue\n",
    "            entity_set.add(obj[0])\n",
    "            entity_triples.add((obj[0], \"label\", obj[1]))\n",
    "            entity_triples.add((obj[0], \"type\", obj[2]))\n",
    "            \n",
    "            triples.add((sbj[0], rel, obj[0]))\n",
    "\n",
    "    return triples, entity_triples, entity_set, relation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction time!\n",
    "for line in data:\n",
    "    inputs = line[\"Chunk text\"] \n",
    "    model_inputs = tokenizer(inputs, max_length=1024, padding=True, truncation=True, return_tensors = 'pt')\n",
    "    #print(model_inputs['input_ids'].size())\n",
    "    generated_tokens = model.generate(\n",
    "                            model_inputs[\"input_ids\"].to('cuda'),\n",
    "                            attention_mask=model_inputs[\"attention_mask\"].to('cuda'),\n",
    "                            **gen_kwargs,\n",
    "                            )\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "    triples, entity_triples, entity_set, relation_set = extract_triples(decoded_preds)\n",
    "    line[\"Extracted Triples\"] = list(triples)\n",
    "    line[\"Entity Triples\"] = list(entity_triples)\n",
    "    line[\"Extracted Entities\"] = list(entity_set)\n",
    "    line[\"Extracted Relations\"] = list(relation_set)\n",
    "    #print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the data in a jsonl file\n",
    "with open('data/preprocessed_data_with_KnowGL_extracted_triples.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for line in data:\n",
    "        line = json.dump(line, f, ensure_ascii=False)\n",
    "        f.write(f'{line}\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get Wikidata IDs for KnowGL extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entities: 3109\n",
      "Total number of relations: 193\n"
     ]
    }
   ],
   "source": [
    "all_entities = set() # to avoid duplicates and reduce the number of API calls\n",
    "all_relations = set() # to avoid duplicates and reduce the number of API calls\n",
    "for line in data:\n",
    "    entities = line[\"Extracted Entities\"]\n",
    "    relations = line[\"Extracted Relations\"]\n",
    "    for ent in entities:\n",
    "        all_entities.add(ent)\n",
    "    for rel in relations:\n",
    "        all_relations.add(rel)\n",
    "print(f\"Total number of entities: {len(all_entities)}\")\n",
    "print(f\"Total number of relations: {len(all_relations)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entity types: 1092\n"
     ]
    }
   ],
   "source": [
    "all_types = set() # to avoid duplicates and reduce the number of API calls\n",
    "for line in data:\n",
    "    entity_triples = line[\"Entity Triples\"]\n",
    "    for triple in entity_triples:\n",
    "        if triple[1] == \"type\":\n",
    "            ent_type = triple[2].strip()\n",
    "            all_types.add(ent_type)\n",
    "print(f\"Total number of entity types: {len(all_types)}\")\n",
    "\n",
    "all_types = sorted(all_types) # to have a consistent order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get wikidata IDs\n",
    "def call_wiki_api(item, item_type='entity'):\n",
    "  if item_type == 'entity':\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&language=en&format=json\"\n",
    "  if item_type == 'property':\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&type=property&language=en&format=json\"\n",
    "  try:\n",
    "    data = requests.get(url).json()\n",
    "    # Return the first id (Could upgrade this in the future)\n",
    "    return data['search'][0]['id']\n",
    "  except:\n",
    "    return 'no-wikiID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the API for all entities and relations to get their wikidata IDs  \n",
    "rel_dict = {}\n",
    "for rel in sorted(all_relations):\n",
    "    rel_dict[rel] = call_wiki_api(rel, 'property')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dictionary\n",
    "with open('data/knowGL_wiki_relations_dict.json', 'w') as f:\n",
    "  json.dump(rel_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's entities' turn\n",
    "ent_dict = {}\n",
    "for ent in sorted(all_entities):\n",
    "    ent_dict[ent] = call_wiki_api(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dictionary\n",
    "with open('data/knowGL_wiki_entities_dict.json', 'w') as f:\n",
    "  json.dump(ent_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's get the wikidata IDs for predicted types\n",
    "type_dict = {}\n",
    "for ent_type in sorted(all_types):\n",
    "    type_dict[ent_type] = call_wiki_api(ent_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dictionary\n",
    "with open('data/knowGL_wiki_types_dict.json', 'w') as f:\n",
    "  json.dump(type_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's triples' turn\n",
    "for line in data:\n",
    "    wikidata_triple_list = []\n",
    "    triple_list = line['Extracted Triples']\n",
    "    ent_triples = line['Entity Triples']\n",
    "    for triple in triple_list:\n",
    "        subj = ent_dict[triple[0]]\n",
    "        rel = rel_dict[triple[1]]\n",
    "        obj = ent_dict[triple[2]]\n",
    "        wiki_triple = [subj, rel, obj]\n",
    "        #print(wiki_triple)\n",
    "        if 'no-wikiID' not in wiki_triple:\n",
    "            wikidata_triple_list.append(wiki_triple)\n",
    "    for triple in ent_triples:\n",
    "        if triple[1] == 'label':\n",
    "            continue\n",
    "        subj = ent_dict[triple[0]]\n",
    "        obj = triple[2]\n",
    "        if obj not in type_dict:\n",
    "            obj = 'no-wikiID'\n",
    "        else:\n",
    "            obj = type_dict[triple[2]] \n",
    "        wiki_triple = [subj, 'P31', obj]\n",
    "        if 'no-wikiID' not in wiki_triple:\n",
    "            wikidata_triple_list.append(wiki_triple)  \n",
    "    line['Wikidata Triples'] = sorted(wikidata_triple_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the data\n",
    "with open('data/preprocessed_data_with_KnowGL_extracted_triples_plus_wikidata.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for line in data:\n",
    "        line = json.dump(line, f, ensure_ascii=False)\n",
    "        f.write(f'{line}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
