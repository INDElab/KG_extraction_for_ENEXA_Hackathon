{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Extraction Notebook\n",
    "\n",
    "##### In this notebook, we:\n",
    "1. use the latest version of Python, Pytorch and Transformers unlike the Data Notebook.\n",
    "2. load the data that we preprocessed in the previous notebook.\n",
    "3. set the generation parameters for the model\n",
    "4. define a post-processing function to shape the triples from the REBEL output\n",
    "5. execute the extraction\n",
    "6. save all the extracted triples, entities and relations separately in a jsonl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.9\n",
      "Pythorch version:  2.0.1\n",
      "Transformers version:  4.30.1\n"
     ]
    }
   ],
   "source": [
    "#REBEL environment details:\n",
    "!python --version\n",
    "import torch\n",
    "print('Pythorch version: ', torch.__version__)\n",
    "import transformers\n",
    "print('Transformers version: ', transformers.__version__)\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data instances:  917\n"
     ]
    }
   ],
   "source": [
    "# Let's start by loading the data we preprocessed in the previous notebook.\n",
    "data = []\n",
    "with open('data/preprocessed_data_for_extraction.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.rstrip('None\\n')))\n",
    "print('Number of data instances: ', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we call the tokenizer and the model from the HuggingFace library\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set the generation parameters for the model\n",
    "gen_kwargs = {\n",
    "    \"max_length\": 1024,\n",
    "    \"length_penalty\": 0,\n",
    "    \"num_beams\": 10, # 10 beams is NOT the default value but we opted for it to get more diverse results\n",
    "    \"num_return_sequences\": 10, # 10 sequences is NOT the default value but we opted for it to get long tail triple extraction\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a post-processing function to shape the triples from the REBEL output\n",
    "def extract_triples(text):\n",
    "    triplets = []\n",
    "    relation, subject, relation, object_ = '', '', '', ''\n",
    "    text = text.strip()\n",
    "    current = 'x'\n",
    "    for token in text.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").split():\n",
    "        if token == \"<triplet>\":\n",
    "            current = 't'\n",
    "            if relation != '':\n",
    "                triplets.append((subject.strip(), relation.strip(), object_.strip()))\n",
    "                relation = ''\n",
    "            subject = ''\n",
    "        elif token == \"<subj>\":\n",
    "            current = 's'\n",
    "            if relation != '':\n",
    "                triplets.append((subject.strip(), relation.strip(), object_.strip()))\n",
    "            object_ = ''\n",
    "        elif token == \"<obj>\":\n",
    "            current = 'o'\n",
    "            relation = ''\n",
    "        else:\n",
    "            if current == 't':\n",
    "                subject += ' ' + token\n",
    "            elif current == 's':\n",
    "                object_ += ' ' + token\n",
    "            elif current == 'o':\n",
    "                relation += ' ' + token\n",
    "    if subject != '' and relation != '' and object_ != '':\n",
    "        triplets.append((subject.strip(), relation.strip(), object_.strip()))\n",
    "    return triplets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction time!\n",
    "for line in data:\n",
    "    triple_set = set() # we save the triples in a set to avoid duplicates\n",
    "    entity_set = set() # we save the entities and relations separately\n",
    "    relation_set = set()\n",
    "    inputs = line[\"Chunk text\"] \n",
    "    model_inputs = tokenizer(inputs, max_length=1024, padding=True, truncation=True, return_tensors = 'pt')\n",
    "    #print(model_inputs['input_ids'].size())\n",
    "    generated_tokens = model.generate(\n",
    "                            model_inputs[\"input_ids\"].to('cuda'),\n",
    "                            attention_mask=model_inputs[\"attention_mask\"].to('cuda'),\n",
    "                            **gen_kwargs,\n",
    "                            )\n",
    "    decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "    for pred in decoded_preds:\n",
    "        #print(pred)\t\n",
    "        triples = extract_triples(pred)\n",
    "        for triple in triples:\n",
    "            triple_set.add(triple)\n",
    "            subj = triple[0]\n",
    "            entity_set.add(subj)\n",
    "            rel = triple[1]\n",
    "            relation_set.add(rel)\n",
    "            obj = triple[2]\n",
    "            entity_set.add(obj)\n",
    "    #print(triple_set )\n",
    "    line[\"Extracted Triples\"] = list(triple_set)\n",
    "    line[\"Extracted Entities\"] = list(entity_set)\n",
    "    line[\"Extracted Relations\"] = list(relation_set)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the data in a jsonl file\n",
    "with open('data/preprocessed_data_with_REBEL_extracted_triples.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for line in data:\n",
    "        line = json.dump(line, f, ensure_ascii=False)\n",
    "        f.write(f'{line}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
