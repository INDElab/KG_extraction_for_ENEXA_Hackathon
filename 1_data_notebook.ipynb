{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Data Notebook\n",
    "\n",
    "##### In this notebook:\n",
    "1. We collect the text from the selected Wikipedia pages using the BeautifulSoup (bs4) library.\n",
    "2. We use nltk sent_tokenize function to divide the text body into sentence. \n",
    "    * divide_chunks function creates two-sentence-lenght chunks. \n",
    "    * Since we cut the whole Wikipedia page into two sentence, we have increased ambiguity problem. For example, a chunk starts with a pronoun refering to an entity from the previous chunk. \n",
    "3. It is almost impossible for the generative extraction model to disambiguate a pronoun referring an entity out of the given context. Therefore, we use a pre-trained coreferance resolution model. We use crosslingual_coreference library, and particularly xlm_roberta model.\n",
    "4. We save the resulting data file: 'data/preprocessed_data_for_extraction.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.8.16\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/finapolat/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/finapolat/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "/home/finapolat/miniconda3/envs/hackathon_enexa/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import Modules\n",
    "from bs4 import *\n",
    "import requests\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import re\n",
    "import json\n",
    "from crosslingual_coreference import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we'll use this function to chunk the text into smaller pieces\n",
    "def divide_chunks(l, n):\n",
    "      \n",
    "    # looping till length l\n",
    "    for i in range(0, len(l), n): \n",
    "        yield l[i:i + n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "models/crosslingual-coreference/xlm-roberta-base/model.tar.gz: 851072KB [00:52, 16181.64KB/s]                            \n",
      "Downloading: 8.68MB [00:00, 15.0MB/s]\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# We use predictor for coreference resolution, and choose xlm-roberta for accuracy.\n",
    "predictor = Predictor(\n",
    "    language=\"en_core_web_sm\", device=-1, model_name=\"xlm_roberta\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fina was born in Bulgaria, but Fina lives in Utrecht now.\n",
      "        Fina has a daughter called Iris. \n",
      "        Fina's daughter is 5 years old.\n",
      "        a daughter called Iris is a very good girl.\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "# Before we start collection text, lets see how the coreference resolution model works\n",
    "text = (\"\"\"Fina was born in Bulgaria, but she lives in Utrecht now.\n",
    "        She has a daughter called Iris. \n",
    "        Her daughter is 5 years old.\n",
    "        She is a very good girl.\n",
    "        \"\"\")\t\n",
    "print(predictor.predict(text)[\"resolved_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The list of the Wikipedia urls we want to collect text from\n",
    "urls = [\"https://en.wikipedia.org/wiki/Adidas\",\n",
    "        \"https://en.wikipedia.org/wiki/Zalando\",\n",
    "        \"https://en.wikipedia.org/wiki/Phoenix_Pharmahandel\",\n",
    "        \"https://en.wikipedia.org/wiki/DATEV\",\n",
    "        \"https://en.wikipedia.org/wiki/BASF\",\n",
    "        \"https://en.wikipedia.org/wiki/Just_Eat_Takeaway.com\",\n",
    "        \"https://en.wikipedia.org/wiki/Syrian_refugee_camps\",\n",
    "        r\"https://en.wikipedia.org/wiki/2022%E2%80%93present_Ukrainian_refugee_crisis\",\n",
    "        \"https://en.wikipedia.org/wiki/List_of_earthquakes_in_California\",\n",
    "        \"https://en.wikipedia.org/wiki/2005_Birmingham_tornado\",\n",
    "        \"https://en.wikipedia.org/wiki/Aftermath_of_the_2011_T%C5%8Dhoku_earthquake_and_tsunami\",\n",
    "        \"https://en.wikipedia.org/wiki/Ozone_depletion\"\n",
    "      \t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch URL Content\n",
    "annotations = []\n",
    "for ind, url in enumerate(urls):\n",
    "    annotation_dict = {} # we'll use this to store the annotations\n",
    "    annotation_dict['Document id'] = ind + 1 # we give each document an id\n",
    "    annotation_dict['Document url'] = url # we store the url\n",
    "    page_name = url.split('/')[-1] # we get the page name from the url\n",
    "    annotation_dict['Document name'] = page_name\n",
    "    page = requests.get(url)\n",
    "    page_content = BeautifulSoup(page.text,'html.parser').select('body')[0]\n",
    "    page_text = []\n",
    "    #print(page_content)\n",
    "    for tag in page_content.find_all(): # we check each tag name\n",
    "        if tag.name==\"p\": # For Paragraph we use p tag\n",
    "            text = tag.text\n",
    "            text = re.sub(r'\\[\\d+\\]', '', text) # Regex that removes the numbers in square bracets\n",
    "            text = text.replace('\\n',  '').replace('\\\\', '').replace('[citation needed]', '')\n",
    "            page_text.append(text)\n",
    "    page_text = ' '.join(page_text).strip()\n",
    "    page_text = sent_tokenize(page_text) # we tokenize the text into sentences\n",
    "    page_text = list(divide_chunks(page_text, 2)) # we make two sentences into a chunk\n",
    "    #print(page_text)\n",
    "    for ind, chunk in enumerate(page_text):\n",
    "        annotation_dict['Chunk id'] = ind + 1\n",
    "        chunk = ' '.join(chunk)\n",
    "        chunk = predictor.predict(chunk)[\"resolved_text\"] # here we use the coref model\n",
    "        annotation_dict['Chunk text'] = chunk\n",
    "        annotations.append(annotation_dict.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/preprocessed_data_for_extraction.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for annotation in annotations:\n",
    "        line = json.dump(annotation, f, ensure_ascii=False)\n",
    "        f.write(f'{line}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "enexa_hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
