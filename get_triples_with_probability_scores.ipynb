{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.9\n",
      "Pythorch version:  2.0.1+cu117\n",
      "Transformers version:  4.30.2\n"
     ]
    }
   ],
   "source": [
    "#environment details:\n",
    "!python --version\n",
    "import torch\n",
    "print('Pythorch version: ', torch.__version__)\n",
    "import transformers\n",
    "print('Transformers version: ', transformers.__version__)\n",
    "import json\n",
    "import requests\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data instances:  917\n"
     ]
    }
   ],
   "source": [
    "# Let's start by loading the data we preprocessed in the previous notebook.\n",
    "data = []\n",
    "with open('data/preprocessed_data_for_extraction.jsonl', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line.rstrip('None\\n')))\n",
    "print('Number of data instances: ', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is a post-processing function to shape the triples from the KnowGL output\n",
    "def extract_triples(pred):\n",
    "    \"\"\" Just one sequence is passed as argument \"\"\"\n",
    "    triples = set()\n",
    "    entity_set = set()\n",
    "    entity_triples = set()\n",
    "    relation_set = set()\n",
    "    pred = pred.replace(\"<s>\", \"\").replace(\"<pad>\", \"\").replace(\"</s>\", \"\").replace(\"[\", \"\").replace(\"]\", \"\").replace(\"(\", \"\").replace(\")\", \"\") # we remove the brackets and parenthesisstrip() # remove special tokens\n",
    "        \n",
    "    if '$' in pred:\n",
    "        pred = pred.split('$')\n",
    "        \n",
    "    else:\n",
    "        pred = [pred]\n",
    "            \n",
    "    pred = [triple.split('|') for triple in pred]   \n",
    "        \n",
    "    for triple in pred:\n",
    "        if len(triple) != 3:\n",
    "            continue\n",
    "        if triple[0] == '' or triple[1] == '' or triple[2] == '':\n",
    "            continue\n",
    "            \n",
    "        sbj = triple[0].split('#')\n",
    "        if len(sbj) != 3:\n",
    "            continue\n",
    "        entity_set.add(sbj[0])\n",
    "        entity_triples.add((sbj[0], \"label\", sbj[1]))\n",
    "        entity_triples.add((sbj[0], \"type\", sbj[2]))\n",
    "            \n",
    "        rel = triple[1]\n",
    "        relation_set.add(rel)\n",
    "            \n",
    "        obj = triple[2].split('#')\n",
    "        if len(obj) != 3:\n",
    "            continue\n",
    "        entity_set.add(obj[0])\n",
    "        entity_triples.add((obj[0], \"label\", obj[1]))\n",
    "        entity_triples.add((obj[0], \"type\", obj[2]))\n",
    "            \n",
    "        triples.add((sbj[0], rel, obj[0]))\n",
    "\n",
    "    return triples, entity_triples, entity_set, relation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we call the tokenizer and the model from the HuggingFace library\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ibm/knowgl-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"ibm/knowgl-large\").to(\"cuda\")\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"Babelscape/rebel-large\")\n",
    "#model = AutoModelForSeq2SeqLM.from_pretrained(\"Babelscape/rebel-large\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we set the generation parameters for the model\n",
    "gen_kwargs = {\n",
    "    \"max_length\": 1024,\n",
    "    \"length_penalty\": 0,\n",
    "    \"num_beams\": 10, # 10 beams is NOT the default value but we opted for it to get more diverse results\n",
    "    \"num_return_sequences\": 10, # 10 sequences is NOT the default value but we opted for it to get long tail triple extraction\n",
    "    \"return_dict_in_generate\": True, \n",
    "    \"output_scores\": True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction time!\n",
    "all_entities = set()\n",
    "all_relations = set()\n",
    "all_types = set()\n",
    "\n",
    "#for line in data[:5]:\n",
    "for line in data:\n",
    "    inputs = line[\"Chunk text\"] \n",
    "    model_inputs = tokenizer(inputs, max_length=1024, padding=True, truncation=True, return_tensors = 'pt')\n",
    "    #print(model_inputs['input_ids'].size())\n",
    "    outputs = model.generate(\n",
    "                            model_inputs[\"input_ids\"].to('cuda'),\n",
    "                            attention_mask=model_inputs[\"attention_mask\"].to('cuda'),\n",
    "                            **gen_kwargs,\n",
    "                            )\n",
    "    #decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=False)\n",
    "    transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, \n",
    "                                                        outputs.beam_indices, normalize_logits=False)\n",
    "    \n",
    "    input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
    "    output_length = input_length + np.sum(transition_scores.cpu().numpy() < 0, axis=1)\n",
    "    length_penalty = model.generation_config.length_penalty\n",
    "    reconstructed_scores = transition_scores.cpu().sum(axis=1) / (output_length**length_penalty)\n",
    "    \n",
    "    all_triples_dict = dict()\n",
    "    all_entity_triples_dict = dict()\n",
    "    entity_set = set()\n",
    "    relation_set = set()\n",
    "    type_set = set()\n",
    "    for seq, seq_score, prob in zip(outputs.sequences, reconstructed_scores, np.exp(reconstructed_scores)):\n",
    "        pred = tokenizer.decode(seq, skip_special_tokens=False)\n",
    "        #print(f'test:{pred}')\n",
    "        triples, entity_triples, entities, relations = extract_triples(pred)\n",
    "        triple_dict = dict()\n",
    "        entity_triple_dict = dict()\n",
    "        for triple in triples:\n",
    "            triple_dict[triple] = {'prob': float(f'{prob:.2f}'), 'log_prob': float(f'{seq_score:.2f}')}\n",
    "        all_triples_dict.update(triple_dict)\n",
    "            \n",
    "        for entity_triple in entity_triples:\n",
    "            if entity_triple[1] == 'type':\n",
    "                type_set.add(entity_triple[2])\n",
    "            entity_triple_dict[entity_triple] = {'prob': float(f'{prob:.2f}'), 'log_prob': float(f'{seq_score:.2f}')}\n",
    "        all_entity_triples_dict.update(entity_triple_dict)\n",
    "            \n",
    "        for entity in entities:\n",
    "            entity_set.add(entity)\n",
    "            \n",
    "        for relation in relations:  \n",
    "            relation_set.add(relation)\n",
    "    all_entities.update(entity_set)\n",
    "    all_relations.update(relation_set)\n",
    "    all_types.update(type_set)\n",
    "    \n",
    "    line[\"Extracted Triples and Probabilities\"] = list(all_triples_dict.items())\n",
    "    line[\"Entity Triples and Probabilities\"] = list(all_entity_triples_dict.items())\n",
    "    line[\"Extracted Entities\"] = list(entity_set)\n",
    "    line[\"Extracted Relations\"] = list(relation_set)\n",
    "    #print(line)    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of entities:  3109\n",
      "Number of relations:  193\n",
      "Number of types:  1097\n"
     ]
    }
   ],
   "source": [
    "print('Number of entities: ', len(all_entities))\n",
    "print('Number of relations: ', len(all_relations))\n",
    "print('Number of types: ', len(all_types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get wikidata IDs\n",
    "def call_wiki_api(item, item_type='entity'):\n",
    "  if item_type == 'entity':\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&language=en&format=json\"\n",
    "  if item_type == 'property':\n",
    "    url = f\"https://www.wikidata.org/w/api.php?action=wbsearchentities&search={item}&type=property&language=en&format=json\"\n",
    "  try:\n",
    "    data = requests.get(url).json()\n",
    "    # Return the first id (Could upgrade this in the future)\n",
    "    return data['search'][0]['id']\n",
    "  except:\n",
    "    return 'no-wikiID'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's call the API for all entities and relations to get their wikidata IDs  \n",
    "rel_dict = {}\n",
    "for rel in sorted(all_relations):\n",
    "    rel_dict[rel] = call_wiki_api(rel, 'property')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dictionary\n",
    "with open('data/knowGL_with_probabilities_wiki_relations_dict.json', 'w') as f:\n",
    "  json.dump(rel_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's entities' turn\n",
    "ent_dict = {}\n",
    "for ent in sorted(all_entities):\n",
    "    ent_dict[ent] = call_wiki_api(ent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dictionary\n",
    "with open('data/knowGL_with_probabilities_wiki_entities_dict.json', 'w') as f:\n",
    "  json.dump(ent_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It's entity types' turn\n",
    "type_dict = {}\n",
    "for tpy in sorted(all_types):\n",
    "    \n",
    "    type_dict[tpy] = call_wiki_api(tpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the dictionary\n",
    "with open('data/knowGL_with_probabilities_wiki_types_dict.json', 'w') as f:\n",
    "  json.dump(type_dict, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We got all Wikidata ids, it is time to build the triples with Wikidata ids\n",
    "# We will use the saved dictionaries to get to add sameAs triples later but at this stage we need one lookup dictionary.\n",
    "\n",
    "one_lookup_dict = ent_dict.copy()\n",
    "one_lookup_dict.update(rel_dict)\n",
    "one_lookup_dict.update(type_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in data:\n",
    "    extracted_triples = line[\"Extracted Triples and Probabilities\"]\n",
    "    ent_type_triples = line[\"Entity Triples and Probabilities\"]\n",
    "    triples_per_line = extracted_triples + ent_type_triples\n",
    "    wiki_triples = []\n",
    "    for triple in triples_per_line:\n",
    "        #print(triple)\n",
    "        #print(triple[1])\n",
    "        subj = one_lookup_dict[triple[0][0]]\n",
    "        if triple[0][1] == 'type':\n",
    "            rel = 'P31'\n",
    "        elif triple[0][1] == 'label':\n",
    "            continue\n",
    "        else:\n",
    "            rel = one_lookup_dict[triple[0][1]]\n",
    "        obj = one_lookup_dict[triple[0][2]]\n",
    "        wiki_triple = (subj, rel, obj), triple[1]\n",
    "        #print(wiki_triple)\n",
    "        wiki_triples.append(wiki_triple)\n",
    "    #print(wiki_triples)\n",
    "    line[\"Wikidata Triples and Probabilities\"] = wiki_triples\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save all the data in a jsonl file\n",
    "with open('data/preprocessed_data_with_KnowGL_extracted_triples_wikidataIDs_and_probabilities.jsonl', 'w', encoding='utf-8') as f:\n",
    "    for line in data:\n",
    "        line = json.dump(line, f, ensure_ascii=False)\n",
    "        f.write(f'{line}\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
